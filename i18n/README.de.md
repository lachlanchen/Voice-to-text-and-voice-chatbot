[English](../README.md) ¬∑ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README.ar.md) ¬∑ [Espa√±ol](README.es.md) ¬∑ [Fran√ßais](README.fr.md) ¬∑ [Êó•Êú¨Ë™û](README.ja.md) ¬∑ [ÌïúÍµ≠Ïñ¥](README.ko.md) ¬∑ [Ti·∫øng Vi·ªát](README.vi.md) ¬∑ [‰∏≠Êñá (ÁÆÄ‰Ωì)](README.zh-Hans.md) ¬∑ [‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ](README.zh-Hant.md) ¬∑ [Deutsch](README.de.md) ¬∑ [–†—É—Å—Å–∫–∏–π](README.ru.md)


# Voice-to-Voice-Chatbot mit Whisper, LLaMA und Groq API

![Python](https://img.shields.io/badge/Python-3.7%2B-3776AB?logo=python&logoColor=white)
![Whisper](https://img.shields.io/badge/STT-OpenAI%20Whisper-1f6feb)
![Groq](https://img.shields.io/badge/LLM-Groq%20LLaMA%203--8B-FF6B35)
![gTTS](https://img.shields.io/badge/TTS-Google%20gTTS-34A853)
![UI](https://img.shields.io/badge/UI-Gradio-F97316)
![Status](https://img.shields.io/badge/Project-Active%20Prototype-2ea44f)

Dieses Projekt ist ein Voice-to-Voice-Chatbot in Echtzeit, der OpenAIs Whisper f√ºr Speech-to-Text-Transkription nutzt, LLaMA 8B √ºber die Groq API zur Antwortgenerierung verwendet und Google Text-to-Speech (gTTS) einsetzt, um Text wieder in Sprache umzuwandeln. Die Chatbot-Oberfl√§che ist mit Gradio erstellt, sodass Nutzer per Spracheingabe oder durch Hochladen von Audiodateien mit dem Bot interagieren k√∂nnen.

## √úberblick

Die App implementiert einen vollst√§ndigen Audio-Konversationszyklus in einem einzigen Python-Skript:

1. Nutzeraudio vom Mikrofon oder aus einer hochgeladenen Datei entgegennehmen.
2. Sprache mit Whisper (Modell `base`) in Text transkribieren.
3. Eine Antwort √ºber Groq (`llama3-8b-8192`) generieren.
4. Antworttext mit gTTS zur√ºck in MP3-Audio umwandeln.
5. Sowohl Antworttext als auch abspielbares Audio in einer Gradio-Web-UI zur√ºckgeben.

### Konversations-Pipeline

| Stufe | Komponente | Ausgabe |
|---|---|---|
| üéôÔ∏è Eingabe | Gradio Audio (Mikrofon/Datei) | Audio-Pfad |
| üìù Transkription | Whisper `base` | Nutzertext |
| üß† Verarbeitung | Groq + `llama3-8b-8192` | Assistententext |
| üîä Synthese | gTTS | MP3-Antwort |
| üñ•Ô∏è Ausgabe | Gradio UI | Text + abspielbares Audio |

## Funktionen

- **Speech-to-Text**: Wandelt gesprochene Sprache mit OpenAIs Whisper-Modell in Text um.
- **KI-generierte Antworten**: Verwendet LLaMA 8B √ºber die Groq API, um auf Basis des transkribierten Textes intelligente Antworten zu erzeugen.
- **Text-to-Speech**: Wandelt generierte Textantworten mit Google Text-to-Speech (gTTS) wieder in Sprache um.
- **Echtzeit-Interaktion**: Der Chatbot arbeitet in Echtzeit und erm√∂glicht die Interaktion √ºber ein Mikrofon oder per Upload von Audiodateien √ºber eine Weboberfl√§che.
- **Einfacher Single-File-Betrieb**: Die gesamte Chatbot-Pipeline ist in `voice_to_voice_chatbot.py` implementiert und damit leicht zu testen.

## Projektstruktur

```text
Voice-to-text-and-voice-chatbot/
‚îú‚îÄ‚îÄ voice_to_voice_chatbot.py               # Main script to run the chatbot
‚îú‚îÄ‚îÄ requirements.txt                        # List of Python dependencies
‚îú‚îÄ‚îÄ i18n/                                   # Localized READMEs (planned/generated in pipeline)
‚îî‚îÄ‚îÄ .auto-readme-work/20260228_230442/      # README automation context/artifacts
```

Referenz der Legacy-Struktur aus der kanonischen README:

```text
voice-to-voice-chatbot/
‚îú‚îÄ‚îÄ voice_to_voice_chatbot.py  # Main script to run the chatbot
‚îú‚îÄ‚îÄ requirements.txt           # List of Python dependencies
‚îú‚îÄ‚îÄ README.md                  # Project documentation
‚îî‚îÄ‚îÄ .gitignore                 # Git ignore file
```

## Voraussetzungen

Bevor du beginnst, stelle sicher, dass folgende Anforderungen erf√ºllt sind:

- Python 3.7 oder h√∂her ist lokal oder in Google Colab installiert.
- Ein Groq API Key. Du kannst dich [hier](https://groq.com/) f√ºr einen API Key registrieren.
- [Google Colab](https://colab.research.google.com/) oder eine lokale Python-Umgebung mit den erforderlichen Bibliotheken.
- Internetzugang f√ºr:
  - den initialen Download des Whisper-Modells.
  - Groq API-Aufrufe.
  - gTTS-Audiogenerierung.

### Anforderungen auf einen Blick

| Anforderung | Warum sie ben√∂tigt wird |
|---|---|
| Python `3.7+` | Laufzeit f√ºr das Chatbot-Skript und seine Abh√§ngigkeiten |
| Groq API Key | Authentifizierter Zugriff auf LLaMA-Modellinferenz |
| Colab oder lokale Umgebung | Ausf√ºhrungsumgebung f√ºr Gradio + ML-Bibliotheken |
| Internetzugang | Whisper-Modell-Download, Groq-Anfragen, gTTS-Synthese |

## Installation

F√ºhre diese Schritte aus, um das Projekt einzurichten:

1. **Repository klonen:**

```bash
git clone https://github.com/aquibali01/voice-to-voice-chatbot.git
cd voice-to-voice-chatbot
```

2. **Abh√§ngigkeiten installieren:**

Installiere die erforderlichen Python-Bibliotheken:

```bash
pip install -r requirements.txt
```

Alternativ kannst du in Google Colab die Bibliotheken mit folgendem Befehl installieren:

```python
!pip install gradio groq-api openai-whisper gtts
```

## Konfiguration

### Groq API Key einrichten

F√ºge deinen Groq API Key zu den Umgebungsvariablen hinzu:

```bash
export GROQ_API_KEY='your_groq_api_key'
```

In Google Colab kannst du den API Key so setzen:

```python
import os
os.environ['GROQ_API_KEY'] = 'your_groq_api_key'
```

### Wichtiger Laufzeithinweis (aktuelles Code-Verhalten)

Das aktuelle Skript initialisiert den Groq-Client mit einem fest codierten Platzhalter:

```python
client = Groq(api_key="your_groq_api_key")
```

Wenn du nur `GROQ_API_KEY` in den Umgebungsvariablen setzt, musst du das Skript so anpassen, dass es aus `os.environ` liest, oder den Platzhalter direkt ersetzen, sonst schl√§gt die API-Authentifizierung fehl.

## Verwendung

Um den Chatbot zu starten, f√ºhre das Hauptskript aus:

```bash
python voice_to_voice_chatbot.py
```

Oder in Google Colab:

Kopiere das Skript in eine Codezelle und f√ºhre sie aus.

Die Gradio-Oberfl√§che startet und erm√∂glicht dir die Interaktion mit dem Chatbot.

### Interaktion mit dem Chatbot

- **Mikrofon verwenden**: Sprich direkt in das Mikrofon. Der Chatbot transkribiert deine Sprache, generiert eine Antwort und spielt sie als Audio ab.
- **Audio hochladen**: Lade eine zuvor aufgenommene Audiodatei hoch. Der Chatbot transkribiert das Audio, erzeugt eine Antwort und wandelt die Antwort wieder in Sprache um.

## Beispiele

### Beispielhafter Sprachfluss

1. Aufnahme: "What are three tips to learn Python quickly?"
2. Whisper transkribiert deinen Eingabetext.
3. Das Groq-LLaMA-Modell generiert eine Antwort.
4. gTTS erzeugt eine MP3-Antwort.
5. Gradio zeigt Antworttext und Audiowiedergabe an.

### Beispielhafter Ausf√ºhrungsbefehl

```bash
python voice_to_voice_chatbot.py
```

Erwartetes Ergebnis: Eine lokale Gradio-App √∂ffnet sich im Browser mit einem Audioeingang und zwei Ausgaben (Text + Audio).

## Entwicklungshinweise

- Hauptfunktion der Pipeline: `chatbot_pipeline(audio_path)`.
- Das Whisper-Modell wird beim Start geladen: `whisper.load_model("base")`.
- Tempor√§re MP3-Dateien werden √ºber `NamedTemporaryFile(..., delete=False)` erstellt.
- Die Fehlerbehandlung gibt derzeit `(str(e), None)` an die UI zur√ºck.
- Die Abh√§ngigkeiten in `requirements.txt` enthalten sowohl `whisper` als auch `openai-whisper`; je nach Umgebung kann das redundant sein.

## Fehlerbehebung

### H√§ufige Probleme

`ModuleNotFoundError`: Stelle sicher, dass du die korrekte Version des Whisper-Moduls installiert hast mit

```python
!pip install -U openai-whisper
```

`Groq API Key Error`: Pr√ºfe deinen API Key und stelle sicher, dass er korrekt in den Umgebungsvariablen gesetzt ist.

Zus√§tzliche Fehlerbehebung:

- Wenn die App sofort mit Authentifizierungsfehlern fehlschl√§gt, pr√ºfe den fest codierten Wert `api_key="your_groq_api_key"` in `voice_to_voice_chatbot.py`.
- Wenn keine Mikrofonaufnahme verf√ºgbar ist, lade zuerst eine Audiodatei hoch, um die STT -> LLM -> TTS-Pipeline zu validieren.
- Wenn das Antwort-Audio leer ist, √ºberpr√ºfe den ausgehenden Netzwerkzugriff f√ºr gTTS.

### Schnelle Diagnose-Checkliste

| Pr√ºfung | Validierung |
|---|---|
| API-Key-Anbindung | `Groq(api_key=...)` ist nicht als Platzhalter belassen |
| Whisper-Installation | `openai-whisper` l√§sst sich fehlerfrei importieren |
| Netzwerkpfad | Ausgehender Zugriff f√ºr Groq + gTTS verf√ºgbar |
| Audioquelle | Mikrofonberechtigungen aktiv oder Dateiupload funktioniert |

## Roadmap

- Groq API Key standardm√§√üig direkt aus Umgebungsvariablen lesen.
- Tests f√ºr Pipeline-Hilfsfunktionen hinzuf√ºgen.
- Optionale Modell-/Konfigurationseinstellungen √ºber Umgebungsvariablen oder CLI-Argumente hinzuf√ºgen.
- i18n-README-Dateien unter `i18n/` erg√§nzen, passend zu den Sprach-Navigationslinks.
- Deployment-Optionen hinzuf√ºgen (zum Beispiel Docker oder Hugging Face Spaces).

## Mitwirken

Beitr√§ge sind willkommen. Bitte fork dieses Repository, erstelle einen neuen Branch und sende einen Pull Request mit deinen √Ñnderungen.

Vorgeschlagener Beitragsablauf:

1. Repository forken und klonen.
2. Einen Feature-Branch erstellen.
3. √Ñnderungen vornehmen und testen.
4. Pull Request mit klarer Beschreibung √∂ffnen.

## Support

Im aktuellen Repository-Inhalt wurden keine expliziten Spenden-/Sponsoring-Links gefunden. Falls Maintainer Support-Kan√§le erg√§nzen, sollten sie hier aufgef√ºhrt werden.

## Lizenz

Dieses Projekt ist unter der MIT License lizenziert. Siehe die LICENSE-Datei f√ºr Details.

Annahme: Eine `LICENSE`-Datei ist vorgesehen, aber in diesem Repository-Snapshot m√∂glicherweise noch nicht vorhanden.
